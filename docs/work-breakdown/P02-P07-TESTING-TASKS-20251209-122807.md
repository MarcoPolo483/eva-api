# P07 Task List - Test Coverage Improvement

**Agent**: P07 (Testing Agent / Frontend Developer)
**Project**: eva-api Test Coverage to 80%+
**Sprint**: Sprint 1-2
**Status**: Not Started
**Epic**: EPIC-001
**Total Estimated Effort**: 40 hours

---

## Priority 0 (Critical - Must Complete This Week)

### Task P0-1: Fix Authentication Test Fixtures
- **Story**: STORY-001
- **Description**: Update `conftest.py` to create auth headers fixture that properly injects JWT tokens into test requests
- **Effort**: 1 hour
- **Technical Approach**:
  ```python
  # tests/conftest.py
  @pytest.fixture
  def mock_jwt_token() -> str:
      """Generate mock JWT token for testing."""
      return "Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
  
  @pytest.fixture
  def auth_headers(mock_jwt_token: str) -> Dict[str, str]:
      """Create authentication headers for requests."""
      return {
          "Authorization": mock_jwt_token,
          "Content-Type": "application/json"
      }
  ```
- **Files Affected**:
  - `tests/conftest.py` (modify)
- **Expected Outcome**: `auth_headers` fixture available to all tests
- **Validation**:
  - [ ] Fixture creates correct header structure
  - [ ] Headers include valid JWT token
  - [ ] Token format matches authentication middleware expectations
- **Dependencies**: None
- **Status**: [ ] Not Started

### Task P0-2: Update Endpoint Tests to Use Auth Headers
- **Story**: STORY-001
- **Description**: Modify all endpoint tests to use new `auth_headers` fixture
- **Effort**: 1 hour
- **Technical Approach**:
  ```python
  # Before:
  def test_create_space(client):
      response = client.post("/api/v1/spaces", json={...})
  
  # After:
  def test_create_space(client, auth_headers):
      response = client.post(
          "/api/v1/spaces",
          headers=auth_headers,
          json={...}
      )
  ```
- **Files Affected**:
  - `tests/test_spaces.py` (20 tests)
  - `tests/test_documents.py` (10 tests)
  - `tests/test_queries.py` (11 tests)
  - Total: ~41 test functions
- **Expected Outcome**: All endpoint tests pass authentication middleware
- **Validation**:
  - [ ] No more 401 Unauthorized failures on valid requests
  - [ ] Tests return expected status codes (200, 201, 404, 422, etc.)
  - [ ] ~40 tests change from failing to passing
- **Dependencies**: Task P0-1 must complete first
- **Status**: [ ] Not Started

### Task P0-3: Fix GraphQL Integration Import
- **Story**: STORY-001
- **Description**: Fix import statement in GraphQL integration tests
- **Effort**: 15 minutes
- **Technical Approach**:
  ```python
  # Before:
  from eva_api.config import settings
  
  # After:
  from eva_api.config import get_settings
  settings = get_settings()
  ```
- **Files Affected**:
  - `tests/integration/test_graphql_integration.py`
- **Expected Outcome**: GraphQL integration tests can import successfully
- **Validation**:
  - [ ] Import error resolved
  - [ ] Tests can execute (may still fail on logic, but no import errors)
  - [ ] 6 GraphQL integration tests move from error to runnable
- **Dependencies**: None
- **Status**: [ ] Not Started

### Task P0-4: Fix Azure OpenAI Configuration Field Names
- **Story**: STORY-001
- **Description**: Update test configuration to use correct Settings field names
- **Effort**: 30 minutes
- **Technical Approach**:
  ```python
  # Before:
  settings.AZURE_OPENAI_ENDPOINT  # âŒ Wrong
  
  # After:
  settings = get_settings()
  settings.azure_openai_endpoint  # âœ… Correct
  ```
- **Files Affected**:
  - `tests/test_query_service_openai.py` (27 tests)
- **Expected Outcome**: Azure OpenAI tests can initialize without ValueError
- **Validation**:
  - [ ] No more "Settings object has no field" errors
  - [ ] 27 tests move from setup error to runnable
  - [ ] Tests may fail on logic but not on configuration
- **Dependencies**: None
- **Status**: [ ] Not Started

### Task P0-5: Run Full Test Suite and Verify Coverage
- **Story**: STORY-001
- **Description**: Execute complete test suite and generate coverage report
- **Effort**: 15 minutes
- **Technical Approach**:
  ```bash
  cd eva-api
  pytest --cov=src/eva_api --cov-report=term-missing --cov-report=html -v
  ```
- **Files Affected**: None (verification only)
- **Expected Outcome**: Coverage report showing ~68% (up from 52.92%)
- **Validation**:
  - [ ] Coverage >= 68%
  - [ ] All P0 fixes reflected in passing tests
  - [ ] Coverage report generated in `htmlcov/`
  - [ ] No configuration errors remaining
- **Dependencies**: Tasks P0-1, P0-2, P0-3, P0-4 complete
- **Status**: [ ] Not Started

---

## Priority 1 (High - Should Complete This Sprint)

### Task P1-1: Implement Blob Streaming Download
- **Story**: STORY-002
- **Description**: Add async streaming download method to BlobStorageService
- **Effort**: 1.5 hours
- **Technical Approach**:
  ```python
  from typing import AsyncIterator
  
  async def download_document_streaming(
      self, space_id: str, document_id: str
  ) -> AsyncIterator[bytes]:
      """Stream document content in chunks for efficient large file downloads."""
      blob_client = self._get_blob_client(space_id, document_id)
      
      try:
          stream = await blob_client.download_blob()
          async for chunk in stream.chunks():
              yield chunk
      except ResourceNotFoundError:
          raise FileNotFoundError(f"Document {document_id} not found")
  ```
- **Files Affected**:
  - `src/eva_api/services/blob_service.py` (add method)
  - `tests/test_blob_storage_service.py` (verify existing test passes)
- **Expected Outcome**: Streaming download method functional, 3 tests passing
- **Validation**:
  - [ ] Method returns AsyncIterator[bytes]
  - [ ] Chunks stream efficiently (don't load entire file in memory)
  - [ ] FileNotFoundError raised for missing documents
  - [ ] Test `test_download_document_streaming` passes
- **Dependencies**: None
- **Status**: [ ] Not Started

### Task P1-2: Implement Blob Batch Delete
- **Story**: STORY-002
- **Description**: Add batch delete method for multiple documents
- **Effort**: 1 hour
- **Technical Approach**:
  ```python
  async def delete_documents_batch(
      self, space_id: str, document_ids: List[str]
  ) -> Dict[str, Any]:
      """Delete multiple documents in batch operation."""
      results = {"deleted": [], "failed": []}
      
      for doc_id in document_ids:
          try:
              await self.delete_document(space_id, doc_id)
              results["deleted"].append(doc_id)
          except Exception as e:
              results["failed"].append({"id": doc_id, "error": str(e)})
      
      return results
  ```
- **Files Affected**:
  - `src/eva_api/services/blob_service.py` (add method)
  - `tests/test_blob_storage_service.py` (verify test passes)
- **Expected Outcome**: Batch delete functional, 1 test passing
- **Validation**:
  - [ ] Returns dict with deleted/failed lists
  - [ ] Partial failures don't stop batch processing
  - [ ] Test `test_delete_document_batch` passes
- **Dependencies**: None
- **Status**: [ ] Not Started

### Task P1-3: Implement SAS URL Generation (Download)
- **Story**: STORY-002
- **Description**: Add method to generate short-lived download URLs
- **Effort**: 1 hour
- **Technical Approach**:
  ```python
  from azure.storage.blob import generate_blob_sas, BlobSasPermissions
  from datetime import datetime, timedelta
  
  async def generate_download_url(
      self, space_id: str, document_id: str, expiry_hours: int = 1
  ) -> str:
      """Generate temporary download URL with SAS token."""
      blob_client = self._get_blob_client(space_id, document_id)
      
      sas_token = generate_blob_sas(
          account_name=self.account_name,
          container_name=self.container_name,
          blob_name=f"{space_id}/{document_id}",
          account_key=self.account_key,
          permission=BlobSasPermissions(read=True),
          expiry=datetime.utcnow() + timedelta(hours=expiry_hours)
      )
      
      return f"{blob_client.url}?{sas_token}"
  ```
- **Files Affected**:
  - `src/eva_api/services/blob_service.py` (add method)
  - `tests/test_blob_storage_service.py` (verify test passes)
- **Expected Outcome**: Download URL generation working, 1 test passing
- **Validation**:
  - [ ] Returns valid URL with SAS token
  - [ ] Token expires after specified hours
  - [ ] Read-only permissions enforced
  - [ ] Test `test_generate_sas_token` passes
- **Dependencies**: None
- **Status**: [ ] Not Started

### Task P1-4: Implement SAS URL Generation (Upload)
- **Story**: STORY-002
- **Description**: Add method to generate short-lived upload URLs
- **Effort**: 30 minutes
- **Technical Approach**:
  ```python
  async def generate_upload_url(
      self, space_id: str, document_id: str, expiry_hours: int = 1
  ) -> str:
      """Generate temporary upload URL with write permissions."""
      # Similar to download but with write permissions
      permission=BlobSasPermissions(write=True, create=True)
  ```
- **Files Affected**:
  - `src/eva_api/services/blob_service.py` (add method)
  - `tests/test_blob_storage_service.py` (verify test passes)
- **Expected Outcome**: Upload URL generation working, 1 test passing
- **Validation**:
  - [ ] Returns valid URL with SAS token
  - [ ] Write/create permissions granted
  - [ ] Test `test_generate_sas_token_custom_permissions` passes
- **Dependencies**: Task P1-3 (similar implementation)
- **Status**: [ ] Not Started

### Task P1-5: Implement Cosmos Space Update/Delete
- **Story**: STORY-003
- **Description**: Add update and delete methods for space management
- **Effort**: 2 hours
- **Technical Approach**:
  ```python
  async def update_space(
      self, space_id: str, updates: Dict[str, Any]
  ) -> Dict[str, Any]:
      """Update space metadata."""
      space = await self.get_space(space_id)
      if not space:
          raise ValueError(f"Space {space_id} not found")
      
      space.update(updates)
      space["updated_at"] = datetime.utcnow().isoformat()
      
      result = await self.spaces_container.upsert_item(space)
      return result
  
  async def delete_space(self, space_id: str) -> bool:
      """Soft delete space (mark as deleted)."""
      space = await self.get_space(space_id)
      if not space:
          return False
      
      space["deleted_at"] = datetime.utcnow().isoformat()
      space["is_deleted"] = True
      await self.spaces_container.upsert_item(space)
      return True
  ```
- **Files Affected**:
  - `src/eva_api/services/cosmos_service.py` (add methods)
  - `tests/test_cosmos_db_service.py` (verify tests pass)
- **Expected Outcome**: Space management functional, 2 tests passing
- **Validation**:
  - [ ] Update preserves existing fields
  - [ ] Delete is soft delete (not permanent)
  - [ ] Tests `test_update_space`, `test_delete_space` pass
- **Dependencies**: None
- **Status**: [ ] Not Started

### Task P1-6: Implement Cosmos Document Metadata CRUD
- **Story**: STORY-003
- **Description**: Add create/delete methods for document metadata
- **Effort**: 2 hours
- **Technical Approach**:
  ```python
  async def create_document_metadata(
      self, space_id: str, metadata: Dict[str, Any]
  ) -> Dict[str, Any]:
      """Create document metadata entry."""
      doc = {
          "id": metadata.get("id", str(uuid.uuid4())),
          "space_id": space_id,
          "filename": metadata["filename"],
          "created_at": datetime.utcnow().isoformat(),
          **metadata
      }
      result = await self.documents_container.create_item(doc)
      return result
  
  async def delete_document_metadata(
      self, space_id: str, document_id: str
  ) -> bool:
      """Delete document metadata."""
      await self.documents_container.delete_item(
          item=document_id,
          partition_key=space_id
      )
      return True
  ```
- **Files Affected**:
  - `src/eva_api/services/cosmos_service.py` (add methods)
  - `tests/test_cosmos_db_service.py` (verify tests pass)
- **Expected Outcome**: Document metadata CRUD functional, 2 tests passing
- **Validation**:
  - [ ] Create generates UUID if not provided
  - [ ] Delete respects partition key
  - [ ] Tests `test_create_document_metadata`, `test_delete_document_metadata` pass
- **Dependencies**: None
- **Status**: [ ] Not Started

### Task P1-7: Implement Cosmos Document Query Methods
- **Story**: STORY-003
- **Description**: Add query methods for documents
- **Effort**: 2 hours
- **Technical Approach**:
  ```python
  async def query_documents_by_space(
      self, space_id: str, filter: Optional[Dict] = None
  ) -> List[Dict[str, Any]]:
      """Query documents within a space (single-partition)."""
      query = "SELECT * FROM c WHERE c.space_id = @space_id"
      params = [{"name": "@space_id", "value": space_id}]
      
      if filter:
          # Add filter conditions
          pass
      
      items = self.documents_container.query_items(
          query=query,
          parameters=params,
          partition_key=space_id
      )
      return [item async for item in items]
  
  async def query_documents(
      self, query: Dict[str, Any]
  ) -> List[Dict[str, Any]]:
      """Query documents across partitions."""
      # Build dynamic query from dict
      # Use cross-partition query
  ```
- **Files Affected**:
  - `src/eva_api/services/cosmos_service.py` (add methods)
  - `tests/test_cosmos_db_service.py` (verify tests pass)
- **Expected Outcome**: Document queries functional, 3 tests passing
- **Validation**:
  - [ ] Single-partition queries efficient (<50ms)
  - [ ] Cross-partition queries functional
  - [ ] Filters apply correctly
  - [ ] Tests `test_query_documents_by_space`, `test_query_documents_with_filter` pass
- **Dependencies**: None
- **Status**: [ ] Not Started

### Task P1-8: Implement Cosmos Query Lifecycle Management
- **Story**: STORY-003
- **Description**: Add methods to track query execution
- **Effort**: 1.5 hours
- **Technical Approach**:
  ```python
  async def create_query(self, query_data: Dict[str, Any]) -> Dict[str, Any]:
      """Create query execution record."""
      query = {
          "id": str(uuid.uuid4()),
          "space_id": query_data["space_id"],
          "question": query_data["question"],
          "status": "pending",
          "created_at": datetime.utcnow().isoformat()
      }
      result = await self.queries_container.create_item(query)
      return result
  
  async def update_query_status(
      self, query_id: str, status: str
  ) -> Dict[str, Any]:
      """Update query execution status."""
      query = await self.queries_container.read_item(query_id, query_id)
      query["status"] = status
      query["updated_at"] = datetime.utcnow().isoformat()
      result = await self.queries_container.upsert_item(query)
      return result
  
  async def get_query_history(
      self, space_id: str, limit: int = 10
  ) -> List[Dict[str, Any]]:
      """Get query history for space."""
      query = """
          SELECT TOP @limit * FROM c 
          WHERE c.space_id = @space_id 
          ORDER BY c.created_at DESC
      """
      items = self.queries_container.query_items(
          query=query,
          parameters=[
              {"name": "@limit", "value": limit},
              {"name": "@space_id", "value": space_id}
          ],
          partition_key=space_id
      )
      return [item async for item in items]
  ```
- **Files Affected**:
  - `src/eva_api/services/cosmos_service.py` (add methods)
  - `tests/test_cosmos_db_service.py` (verify tests pass)
- **Expected Outcome**: Query lifecycle tracking functional, 3 tests passing
- **Validation**:
  - [ ] Queries tracked from creation to completion
  - [ ] Status updates atomic
  - [ ] History ordered by timestamp
  - [ ] Tests `test_create_query`, `test_update_query_status`, `test_query_history_by_space` pass
- **Dependencies**: None
- **Status**: [ ] Not Started

### Task P1-9: Implement Cosmos Cross-Partition Methods
- **Story**: STORY-003
- **Description**: Add cross-partition query utilities
- **Effort**: 1.5 hours
- **Technical Approach**:
  ```python
  async def query_all_documents(
      self, filter: Optional[Dict] = None
  ) -> List[Dict[str, Any]]:
      """Query all documents across all partitions."""
      query = "SELECT * FROM c"
      items = self.documents_container.query_items(
          query=query,
          enable_cross_partition_query=True
      )
      return [item async for item in items]
  
  async def get_total_document_count(self) -> int:
      """Get total document count across all spaces."""
      query = "SELECT VALUE COUNT(1) FROM c"
      items = self.documents_container.query_items(
          query=query,
          enable_cross_partition_query=True
      )
      return await items.__anext__()
  
  async def search_documents_by_filename(
      self, pattern: str
  ) -> List[Dict[str, Any]]:
      """Search documents by filename pattern."""
      query = "SELECT * FROM c WHERE CONTAINS(c.filename, @pattern)"
      items = self.documents_container.query_items(
          query=query,
          parameters=[{"name": "@pattern", "value": pattern}],
          enable_cross_partition_query=True
      )
      return [item async for item in items]
  ```
- **Files Affected**:
  - `src/eva_api/services/cosmos_service.py` (add methods)
  - `tests/test_cosmos_db_service.py` (verify tests pass)
- **Expected Outcome**: Cross-partition queries functional, 3 tests passing
- **Validation**:
  - [ ] Cross-partition flag enabled
  - [ ] Queries return results across all spaces
  - [ ] COUNT aggregation works
  - [ ] Tests `test_query_across_all_spaces`, `test_aggregate_document_count`, `test_search_by_filename_pattern` pass
- **Dependencies**: None
- **Status**: [ ] Not Started

---

## Priority 2 (Medium - Nice to Have This Sprint)

### Task P2-1: Implement Token Estimation
- **Story**: STORY-004
- **Description**: Add tiktoken-based token counting
- **Effort**: 1.5 hours
- **Technical Approach**:
  ```python
  import tiktoken
  
  async def estimate_tokens(self, text: str, model: str = "gpt-4") -> int:
      """Estimate token count for text using tiktoken."""
      try:
          encoding = tiktoken.encoding_for_model(model)
      except KeyError:
          encoding = tiktoken.get_encoding("cl100k_base")
      
      return len(encoding.encode(text))
  ```
- **Files Affected**:
  - `src/eva_api/services/query_service.py` (add method)
  - `requirements.txt` (add tiktoken dependency)
  - `tests/test_query_service_openai.py` (verify tests pass)
- **Expected Outcome**: Token estimation functional, 3 tests passing
- **Validation**:
  - [ ] Accurate token counts for various models
  - [ ] Fallback to cl100k_base for unknown models
  - [ ] Tests `test_estimate_tokens`, etc. pass
- **Dependencies**: Add tiktoken to dependencies
- **Status**: [ ] Not Started

### Task P2-2: Implement OpenAI Retry Logic
- **Story**: STORY-004
- **Description**: Add exponential backoff retry for rate limits
- **Effort**: 1.5 hours
- **Technical Approach**:
  ```python
  from openai import RateLimitError
  import asyncio
  
  async def _call_openai_with_retry(
      self, messages: List[Dict], max_retries: int = 3
  ) -> Dict:
      """Call OpenAI with exponential backoff retry."""
      for attempt in range(max_retries):
          try:
              response = await self.openai_client.chat.completions.create(
                  model=self.settings.azure_openai_deployment,
                  messages=messages
              )
              return response
          except RateLimitError as e:
              if attempt < max_retries - 1:
                  wait_time = 2 ** attempt  # 1s, 2s, 4s
                  await asyncio.sleep(wait_time)
              else:
                  raise
  ```
- **Files Affected**:
  - `src/eva_api/services/query_service.py` (add method)
  - `tests/test_query_service_openai.py` (verify tests pass)
- **Expected Outcome**: Retry logic functional, 3 tests passing
- **Validation**:
  - [ ] Retries on rate limit errors
  - [ ] Exponential backoff implemented
  - [ ] Max retries respected
  - [ ] Tests `test_retry_on_rate_limit`, `test_max_retries_exceeded`, `test_exponential_backoff` pass
- **Dependencies**: None
- **Status**: [ ] Not Started

### Task P2-3: Fix Webhook Event Loop Management
- **Story**: STORY-005
- **Description**: Refactor webhook service to properly handle asyncio lifecycle
- **Effort**: 4 hours
- **Technical Approach**:
  ```python
  class WebhookService:
      def __init__(self):
          self._delivery_queue: Optional[asyncio.Queue] = None
          self._current_loop: Optional[asyncio.AbstractEventLoop] = None
          self._worker_task: Optional[asyncio.Task] = None
      
      async def _ensure_queue(self):
          """Ensure queue is bound to current event loop."""
          current_loop = asyncio.get_running_loop()
          if self._current_loop != current_loop:
              self._delivery_queue = asyncio.Queue()
              self._current_loop = current_loop
          return self._delivery_queue
      
      async def start(self):
          """Start webhook delivery service."""
          queue = await self._ensure_queue()
          self._worker_task = asyncio.create_task(
              self._delivery_worker(queue)
          )
  ```
- **Files Affected**:
  - `src/eva_api/services/webhook_service.py` (refactor)
  - `tests/test_phase3_features.py` (verify no errors)
- **Expected Outcome**: No RuntimeError in tests, clean logs
- **Validation**:
  - [ ] No "bound to different event loop" errors
  - [ ] Queue properly initialized per loop
  - [ ] Tests run without runtime errors
  - [ ] Production deployment successful
- **Dependencies**: None (architectural fix)
- **Status**: [ ] Not Started

### Task P2-4: Set Up Integration Test Environment
- **Story**: STORY-006
- **Description**: Configure integration tests with real Azure credentials
- **Effort**: 2 hours
- **Technical Approach**:
  ```python
  # tests/integration/conftest.py
  @pytest.fixture(scope="session")
  def integration_settings():
      """Use real Azure settings for integration tests."""
      load_dotenv(".env.integration")
      return Settings(
          mock_mode=False,
          cosmos_endpoint=os.getenv("AZURE_COSMOS_ENDPOINT"),
          cosmos_key=os.getenv("AZURE_COSMOS_KEY"),
          blob_connection_string=os.getenv("AZURE_BLOB_CONNECTION_STRING")
      )
  ```
- **Files Affected**:
  - `tests/integration/conftest.py` (create/modify)
  - `.env.integration.example` (create template)
  - `tests/integration/test_blob_integration.py` (update)
  - `tests/integration/test_graphql_integration.py` (update)
- **Expected Outcome**: Integration tests can connect to real Azure
- **Validation**:
  - [ ] Integration tests use separate config
  - [ ] No credentials in code
  - [ ] Tests can run in CI/CD with env vars
- **Dependencies**: Azure dev environment provisioned
- **Status**: [ ] Not Started

### Task P2-5: Fix Blob Integration Tests
- **Story**: STORY-006
- **Description**: Update blob integration tests with proper Azure URLs
- **Effort**: 2 hours
- **Technical Approach**: Use integration_settings fixture, real blob operations
- **Files Affected**:
  - `tests/integration/test_blob_integration.py`
- **Expected Outcome**: 4 blob integration tests passing
- **Validation**:
  - [ ] Upload/download working
  - [ ] SAS URL generation working
  - [ ] Delete operations working
  - [ ] Hierarchical naming tested
- **Dependencies**: Task P2-4
- **Status**: [ ] Not Started

### Task P2-6: Fix GraphQL Integration Tests
- **Story**: STORY-006
- **Description**: Update GraphQL integration tests with proper setup
- **Effort**: 2 hours
- **Technical Approach**: Use integration_settings, real GraphQL queries
- **Files Affected**:
  - `tests/integration/test_graphql_integration.py`
- **Expected Outcome**: 6 GraphQL integration tests passing
- **Validation**:
  - [ ] Space creation/query working
  - [ ] Pagination working
  - [ ] Query submission working
  - [ ] Error handling working
- **Dependencies**: Task P2-4, Task P0-3 (import fix)
- **Status**: [ ] Not Started

---

## Progress Tracker

| Task | Priority | Effort | Status | Coverage Impact | Assignee |
|------|----------|--------|--------|-----------------|----------|
| P0-1 | P0 | 1h | Not Started | +0% (foundation) | P07 |
| P0-2 | P0 | 1h | Not Started | +16% | P07 |
| P0-3 | P0 | 15m | Not Started | +2% | P07 |
| P0-4 | P0 | 30m | Not Started | +3% | P07 |
| P0-5 | P0 | 15m | Not Started | Validation | P07 |
| P1-1 | P1 | 1.5h | Not Started | +1% | P07 |
| P1-2 | P1 | 1h | Not Started | +0.5% | P07 |
| P1-3 | P1 | 1h | Not Started | +0.5% | P07 |
| P1-4 | P1 | 30m | Not Started | +0.5% | P07 |
| P1-5 | P1 | 2h | Not Started | +1% | P07 |
| P1-6 | P1 | 2h | Not Started | +1% | P07 |
| P1-7 | P1 | 2h | Not Started | +2% | P07 |
| P1-8 | P1 | 1.5h | Not Started | +1% | P07 |
| P1-9 | P1 | 1.5h | Not Started | +1% | P07 |
| P2-1 | P2 | 1.5h | Not Started | +1% | P07 |
| P2-2 | P2 | 1.5h | Not Started | +1% | P07 |
| P2-3 | P2 | 4h | Not Started | +0% (stability) | P07 |
| P2-4 | P2 | 2h | Not Started | +0% (foundation) | P07 |
| P2-5 | P2 | 2h | Not Started | +1% | P07 |
| P2-6 | P2 | 2h | Not Started | +1% | P07 |

**Total Effort**: 30.25 hours (P0: 3.5h, P1: 13.5h, P2: 13.25h)
**Expected Coverage Progression**: 52.92% â†’ 68% â†’ 78% â†’ 88%

---

## Notes & Blockers

### Current Blockers
- None (ready to start)

### Dependencies External to P07
- Azure dev environment credentials (for P2 tasks)
- P09 for validation and quality gates
- P11 for deployment after P0 tasks complete

### Recommended Sequence
1. **Week 1**: Complete all P0 tasks (3.5 hours total)
   - This gives immediate +16% coverage boost
   - Unlocks deployment to dev environment
2. **Week 1-2**: Complete P1 tasks (13.5 hours total)
   - Implement missing service methods
   - Achieve ~78% coverage
3. **Week 2-3**: Complete P2 tasks as capacity allows (13.25 hours)
   - Polish and enhance
   - Achieve 88% coverage target

### Testing Strategy
- Run full test suite after each task completion
- Track coverage delta to confirm expected impact
- Flag any unexpected failures immediately
- Keep coverage reports in `htmlcov/` for review

---

**P07 Status**: ðŸ“‹ Ready to begin
**Next Action**: Start with Task P0-1 (authentication fixtures)
**Estimated Sprint 1 Completion**: 17 hours (P0 + P1 tasks)
**Estimated Sprint 2 Completion**: 13 hours (P2 tasks)
